# text_processor.py
import re
import logging
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime

# Cáº¥u hÃ¬nh logging
logger = logging.getLogger(__name__)

class TextProcessor:
    """
    Lá»›p xá»­ lÃ½ vÄƒn báº£n Ä‘áº§u vÃ o, bao gá»“m Ä‘á»c, lÃ m sáº¡ch vÃ  cÃ¡c chiáº¿n lÆ°á»£c phÃ¢n Ä‘oáº¡n thÃ´ng minh.
    """
    def __init__(self):
        self.supported_formats = ['.txt', '.md', '.json']

    def read_file(self, file_path: str) -> str:
        """Äá»c file vÄƒn báº£n vá»›i xá»­ lÃ½ lá»—i."""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"âœ… ÄÃ£ Ä‘á»c thÃ nh cÃ´ng {len(content)} kÃ½ tá»± tá»« {file_path.name}")
            return content
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi Ä‘á»c file: {e}")
            return ""

    def clean_text(self, text: str) -> str:
        """LÃ m sáº¡ch vÄƒn báº£n, loáº¡i bá» khoáº£ng tráº¯ng vÃ  cÃ¡c dÃ²ng trá»‘ng thá»«a."""
        if not text or not text.strip():
            return ""

        text = re.sub(r'\r\n', '\n', text)
        text = re.sub(r'[ \t]+', ' ', text)

        lines = [line.strip() for line in text.splitlines() if line.strip()]
        cleaned_text = '\n'.join(lines)

        logger.info(f"ğŸ§¹ VÄƒn báº£n sau khi lÃ m sáº¡ch: {len(cleaned_text)} kÃ½ tá»±")
        return cleaned_text

    def _split_by_sentence(self, text: str, chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
        """
        Chiáº¿n lÆ°á»£c chunking cÆ¡ báº£n: chia theo cÃ¢u vÃ  ghÃ©p láº¡i.
        ÄÃ¢y lÃ  phÆ°Æ¡ng thá»©c ná»™i bá»™ (private method).
        """
        logger.info(f"Ãp dá»¥ng chiáº¿n lÆ°á»£c chunking theo cÃ¢u (size={chunk_size}, overlap={overlap})...")
        if not text: return []

        from langchain.text_splitter import RecursiveCharacterTextSplitter

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap,
            separators=["\n\n", "\n", ". ", ", ", " ", ""]
        )

        docs = text_splitter.split_text(text)

        chunks = []
        for i, doc_content in enumerate(docs):
            chunks.append({
                "id": f"chunk_sent_{datetime.now().timestamp()}_{i}",
                "content": doc_content,
                "length": len(doc_content)
            })

        logger.info(f"ğŸ“ ÄÃ£ táº¡o {len(chunks)} chunks theo cÃ¢u.")
        return chunks

    def _split_by_law_article(self, text: str, max_chars_per_chunk: int) -> List[Dict[str, Any]]:
        """
        Chiáº¿n lÆ°á»£c chunking thÃ´ng minh: chia theo cáº¥u trÃºc ChÆ°Æ¡ng, Äiá»u cá»§a vÄƒn báº£n luáº­t.
        ÄÃ¢y lÃ  phÆ°Æ¡ng thá»©c ná»™i bá»™ (private method).
        """
        logger.info("Ãp dá»¥ng chiáº¿n lÆ°á»£c chunking thÃ´ng minh theo Äiá»u luáº­t...")

        raw_parts = re.split(r'(ChÆ°Æ¡ng [IVXLCDM\d]+.*?)\n|(Äiá»u \d+\..*?)\n', text)
        parts = [p.strip() for p in raw_parts if p and p.strip()]

        chunks = []
        current_heading = ""
        current_content = ""
        chunk_id_counter = 0

        for part in parts:
            is_heading = part.startswith("ChÆ°Æ¡ng") or part.startswith("Äiá»u")

            if is_heading and current_content:
                full_content = (current_heading + "\n" + current_content).strip()
                chunks.append({
                    "id": f"chunk_law_{datetime.now().timestamp()}_{chunk_id_counter}",
                    "content": full_content,
                    "length": len(full_content),
                    "metadata": {"heading": current_heading}
                })
                chunk_id_counter += 1

                current_heading = part
                current_content = ""
            elif is_heading:
                current_heading = part
            else:
                current_content += "\n" + part

        if current_content:
            full_content = (current_heading + "\n" + current_content).strip()
            chunks.append({
                "id": f"chunk_law_{datetime.now().timestamp()}_{chunk_id_counter}",
                "content": full_content,
                "length": len(full_content),
                "metadata": {"heading": current_heading}
            })

        final_chunks = []
        for chunk in chunks:
            if chunk['length'] > max_chars_per_chunk:
                logger.warning(f"âš ï¸ Chunk '{chunk['metadata']['heading']}' quÃ¡ dÃ i ({chunk['length']} kÃ½ tá»±). Sáº½ chia nhá» hÆ¡n.")
                smaller_chunks_data = self._split_by_sentence(chunk['content'], max_chars_per_chunk, overlap=int(max_chars_per_chunk*0.1))
                final_chunks.extend(smaller_chunks_data)
            else:
                final_chunks.append(chunk)

        logger.info(f"ğŸ“ ÄÃ£ táº¡o {len(final_chunks)} chunks dá»±a trÃªn cáº¥u trÃºc Äiá»u/ChÆ°Æ¡ng.")
        return final_chunks

    def split_into_chunks(self, text: str, chunk_size: int = 1500, overlap: int = 150, strategy: str = "law_article") -> List[Dict[str, Any]]:
        """
        PhÆ°Æ¡ng thá»©c chÃ­nh Ä‘á»ƒ phÃ¢n Ä‘oáº¡n vÄƒn báº£n, cÃ³ thá»ƒ chá»n chiáº¿n lÆ°á»£c.
        """
        if strategy == "law_article":
            return self._split_by_law_article(text, max_chars_per_chunk=chunk_size)
        elif strategy == "sentence":
            return self._split_by_sentence(text, chunk_size, overlap)
        else:
            logger.error(f"âŒ Chiáº¿n lÆ°á»£c chunking khÃ´ng há»£p lá»‡: {strategy}. Sá»­ dá»¥ng 'sentence' lÃ m máº·c Ä‘á»‹nh.")
            return self._split_by_sentence(text, chunk_size, overlap)